---
title: "Applied Statistics Coursework 2025/2026"
subtitle: 'MSc in Statistics 2025/26, Imperial College London'
author: "06051710"
format:
  html:
    toc: true
    highlight: tango
    self-contained: true
    df-print: paged
  pdf: default
format-links: false
editor: 
  markdown: 
    wrap: 72
---

```{=html}
<style type="text/css">
h1{
  font-size: 24pt;
}
h2{
  font-size: 18pt;
}
body{
  font-size: 12pt;
}
</style>
```

```{r setup, include = FALSE, tidy=TRUE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
include_solutions <- TRUE
```

```{r setup2, include=FALSE, tidy=TRUE}
set.seed(17)
require(rmarkdown)
require(knitr)
require(kableExtra)
require("mcmc")
require("ggplot2")
# Put any library imports and other preamble here.
```

# Question 1

One way the independence assumption could be violated is if the survey
participants are in frequent contact with one another. This could mean
that even if one participant does not smoke, they may be breathing in
smoke from the smokers around them, making the number of cigarettes
smoked by others directly affect their rate of lung cancer as well.

Another way the independence assumption could be violated is if the
survey participants are related to one another in some way (at its worst
there could be direct relatives in the sample). This would mean due to
shared genetic characteristics, the probability one participant develops
cancer is not independent of the probability another does.

Finally, the independence assumption could be violated by repeated
sampling. For example, sometimes when filling a form online, a glitch
could occur and record one participant's responses twice. These two rows
are therefore clearly not independent, as knowing the outcome of the
first response perfectly predicts the outcome of the second.

# Question 2

### Part a

Yes, it would be a loss of information, since we now have no way to
differentiate between somebody who smokes 10 cigarettes a day versus 1
cigarette a day.

### Part b

The first likelihood can be calculated as follows: (assuming $S_i = 1$
or $0$ depending on whether respondent i is a smoker or not
respectively)

$$
\begin{aligned}
\mathbb{P}(C_{1:N} | p, q, S_{1:N}) &= \prod_{i=1}^n \mathbb{P}(C_i | p, q, S_i) \quad \text{(by assuming independence)} \\
&= \prod_{i=1}^n p^{S_i C_i} \cdot (1-p)^{S_i (1-C_i)} q^{(1-S_i) C_i} (1-q)^{(1-S_i)(1-C_i)} \\
&= p^{\sum_{i=1}^n S_i C_i} \cdot (1-p)^{\sum_{i=1}^n S_i (1-C_i)} q^{\sum_{i=1}^n (1-S_i) C_i} \cdot (1-q)^{\sum_{i=1}^n (1-S_i)(1-C_i)} \\
&= p^{N_{S,C}} (1-p)^{N_S - N_{S,C}} q^{N_{\neg S, C}} (1-q)^{N_{\neg S} - N_{\neg S, C}}
\end{aligned}
$$

With the summary statistics the likelihood is:

$$
\mathbb{P}(N_{S,C}, N_{\neg S, C} \mid p, q, N_S, N_{\neg S}) = \mathbb{P}(N_{S,C} \mid p, N_S) \cdot \mathbb{P}(N_{\neg S, C} \mid q, N_{\neg S})
$$

Which separates due to the assumption of independence in the trials, so
the number of non smokers with cancer should not affect the number of
smokers with cancer in the sample.

Further, the independence between each participant can let us treat the
distribution as the product of two binomial distributions, leading to
the following:

$$
\mathbb{P}(N_{S,C}, N_{\neg S, C} \mid p, q, N_S, N_{\neg S}) = \binom{N_S}{N_{S,C}} p^{N_{S,C}} (1-p)^{N_S - N_{S,C}} \cdot \binom{N_{\neg S}}{N_{\neg S, C}} q^{N_{\neg S, C}} (1-q)^{N_{\neg S} - N_{\neg S, C}}
$$

If we then find the ratio of the two likelihoods, we find it is
independent of both $p$ and $q$, so (by 6.4.1 in the notes) the
statistic $(N_{S,C}, N_{\neg S, C})$ is a sufficient statistic (so
together $N_{S,C}$ and $N_{\neg S, C}$ are jointly sufficient
statistics).

### Part c

@fig-1 clearly demonstrates how the posterior probability of a smoker
developing lung cancer is higher than the posterior probability of a non
smoker (I sampled from the joint distribution of $p$ and $q$ using
Markov Chain Monte Carlo techniques). Testing the hypothesis that $p>q$
is essentially done in Section D where the competing hypotheses are
represented by different models, however, just visually, @fig-1 suggests
there is high evidence that smoking is linked to lung cancer.

```{r}
#| echo: false
#| label: fig-1
#| fig-cap: "Marginal posterior densities for p and q"

########
#Question 2C
########

library("mcmc")
smoking_dat <- read.csv('smoking.csv')

#Extracting relevant summary statistics

#Number of smokers
Ns <- sum(smoking_dat$cigarettes>0)
#Number of non-smokers
Nns <- 36-Ns 
#Number of smokers with cancer
Nsc <- sum(smoking_dat$cigarettes>0 & smoking_dat$cancer) 
#Number of non smokers with cancer
Nnsc <- sum(smoking_dat$cigarettes==0 & smoking_dat$cancer) 


#will work with log posterior as that is a requirement of metrop (in mcmc library)
log_posterior <- function(theta){
  p<-theta[1]
  q<-theta[2]
  
  #The Uniform Prior on  parameters p and q.
  if (p <= 0 || p >= 1 || q <= 0 || q >= 1) return(-Inf)
  
  #The log of the posterior distribution up to normalisation constant
  log_post <- Nsc*log(p) + (Ns-Nsc)*log(1-p) + Nnsc*log(q) + (Nns - Nnsc)*log(1-q)
  return (log_post)
}

nsmp <- 100000
#started here to demonstrate ignorance of the true distribution
params_initial <- c(0.5,0.5) 

posterior <- metrop(
  obj= log_posterior,
  initial = params_initial,
  nbatch=nsmp
)

posterior_samples <- as.matrix(posterior$batch)
colnames(posterior_samples) <- c("p", "q")

#Plotting the marginal posterior densities for p and q

library(ggplot2)

df <- data.frame(
  value = c(posterior_samples[, "p"], posterior_samples[, "q"]),
  parameter = rep(c("p", "q"), each = nrow(posterior_samples))
)

ggplot(df, aes(x = value, color = parameter)) +
  geom_density(linewidth = 1) +
  labs(
    x = "Value",
    y = "Density"
  ) +
  theme_minimal()




```

### Part d

I calculated the marginal likelihood for each model by integrating the
likelihood multiplied by the prior for each model over the parameter
space. This was done by the following formula:

$\mathbb{P}({N_{s,c}, N_{\neg s, c} } \mid M_i) = \int \mathbb{P}(\boldsymbol{\theta} \mid M_i) \, \mathbb{P}({N_{s,c}, N_{\neg s, c} }\mid \boldsymbol{\theta}, M_i) \mathrm{d}\boldsymbol{\theta}$

where $\boldsymbol{\theta} = (p,q)$, for every model. Additionally, the
likelihood is the same for each model and is as calculated in part b.

Then for $M_1$ the marginal likelihood became:

$$
2 \cdot \binom{N_S}{N_{S,c}} \cdot \binom{N_{\neg S}}{N_{\neg S,c}}
\cdot \int_{0}^{1} q^{\,N_{\neg S,c}} (1-q)^{\,N_{\neg S}-N_{\neg S,c}}
\left( \int_{q}^{1} \rho^{\,N_{S,c}} (1-\rho)^{\,N_{S}-N_{S,c}} \, d\rho \right)
\, dq
$$

where the prior distribution specifying $p>q$ was incorporated into the
limits of the inner integral.

For $M_2$ I found the marginal likelihood to be:

$$
\binom{N_S}{N_{S,c}} \binom{N_{\neg S}}{N_{\neg S,c}}
\int_{0}^{1}
q^{\,N_{\neg S,c}} (1-q)^{\,N_{\neg S}-N_{\neg S,c}}
\cdot
q^{\,N_{S,c}} (1-q)^{\,N_S-N_{S,c}}
\, dq
$$ where the prior was incorporated by taking $p=q$ in the inner
integral (with respect to $p$) and then finding the remaining dirac
function integrated to 1.

Finally, for $M_3$ I found the marginal likelihood in a similar way to
$M_1$ to be:

$$
2 \cdot \binom{N_S}{N_{S,c}} \cdot \binom{N_{\neg S}}{N_{\neg S,c}}
\cdot \int_{0}^{1} q^{\,N_{\neg S,c}} (1-q)^{\,N_{\neg S}-N_{\neg S,c}}
\left( \int_{0}^{q} \rho^{\,N_{S,c}} (1-\rho)^{\,N_{S}-N_{S,c}} \, d\rho \right)
\, dq
$$ Again, representing the prior information through the limits of the
inner integral.

I used numerical integration (specifically the integrate function in R)
to solve for each likelihood. This led to the following likelihoods (for
models 1,2 and 3 respectively).

```{r}
#| echo: false

#########
#Question 2d
#########

#remember to remove the following block before uploading
smoking_dat <- read.csv('smoking.csv')

#Extracting relevant summary statistics

#Number of smokers
Ns <- sum(smoking_dat$cigarettes>0)
#Number of non-smokers
Nns <- 36-Ns 
#Number of smokers with cancer
Nsc <- sum(smoking_dat$cigarettes>0 & smoking_dat$cancer) 
#Number of non smokers with cancer
Nnsc <- sum(smoking_dat$cigarettes==0 & smoking_dat$cancer) 


#Defining f so I can integrate numerically
f <- function(p,q){
  #the normalisation constants are taken outside the integral
  return (p^(Nsc)*(1-p)^(Ns-Nsc)*q^(Nnsc)*(1-q)^(Nns-Nnsc))
}

#Model 1
inner_integral1 <- Vectorize(function(q){
  integrate(function(p) f(p, q), lower = q, upper = 1)$value
})

outer_integral1 <- integrate(inner_integral1, lower = 0, upper = 1)

m1e <- 2*choose(Ns, Nsc)*choose(Nns, Nnsc)*outer_integral1$value




#Model 2

integral2 <- integrate(function(q){q^(Nsc+Nnsc)*(1-q)^(36-Nsc-Nnsc)}, lower=0, upper=1 )
m2e <- choose(Ns, Nsc)*choose(Nns, Nnsc)*integral2$value




#Model 3
inner_integral3 <- Vectorize(function(q){
  integrate(function(p) f(p, q), lower = 0, upper = q)$value
})

outer_integral3 <- integrate(inner_integral3, lower = 0, upper = 1)
m3e <- 2*choose(Ns, Nsc)*choose(Nns, Nnsc)*outer_integral3$value

print(c(m1e, m2e, m3e))

```

To assess the model evidence, I used the posterior odds ratio for each
model. Assuming each model is equally likely a priori, this reduced to
the Bayes factor, which was the ratio of the marginal likelihoods just
calculated.

```{r}
#Bayes Factor
print(m1e/m2e)
print(m1e/m3e)
print(m2e/m3e)
```

Per the Jeffreys scale the evidence for model 1 (i.e. that smoking is
linked to lung cancer) is overwhelming compared to alternative models.
However, even acknowledging that the cutoff points in Jeffreys scale are
largely arbitrary, these ratios are so much larger than the cutoff
points (where \>10 suggests strong evidence), that this data suggests
very strong evidence for the claim smoking is linked to lung cancer.

# Question 3

### Part a

To answer this question, I first noticed that the distribution was
proportional to a location-scale version of a student's t distribution
with location $\mu$, scale $s^2$, and degrees of freedom $\nu$ (below):

$$
f(x) \propto \left( 1 + \frac{(x-\mu)^2}{\nu s^2} \right)^{-(\nu+1)/2}
$$

Specifically, the density is proportional to a student's t in the
following form:

$$
P(L \mid L^*, \Delta, \sigma, \beta)
\propto
\left\{
1 +
\frac{\bigl(L - (L^*+\Delta)\bigr)^2}{(2\beta-1)}
\left(
\frac{\sigma}{\sqrt{2\beta-1}}
\right)^{-2}
\right\}^{-\beta}
$$

$$
\propto \;\text{Student}\!\left(
\nu = 2\beta - 1,\;
\mu = L^* + \Delta,\;
s = \frac{\sigma}{\sqrt{2\beta-1}}
\right)
$$

Then, using the known properties of a t distribution, I then knew for i)
the distribution to be normaliseable, $\nu > 0$. This restricted
$\beta > \frac{1}{2}$

For ii) the distribution to have a defined mean, $\nu > 1$. This
restricted $\beta > 1$.

Finally, for iii) the distribution to have a defined variance,
$\nu > 2$. This restricted $\beta > \frac{3}{2}$.

All of these criteria are necessary for the distribution to be a valid
noise model, as to be a useful model for data variation, it would need a
defined variance (which necessitates a defined mean, and a normaliseable
distribution).

### Part b

Yes, this is a problem in principle, as the sampling distribution should
represent the true data generating process. If a negative value is
impossible in the true data generating process this should therefore be
represented in the sampling distribution.

However, given our data, this should not be a major problem in practice
as the model will likely incorporate the fact that no negative values
were recorded through its shape, bias and noise parameters.

### Part c

The likelihood for the data is going to be normaliseable if the original
density is normaliseable, which the prior restrictions enforce; if
$\beta > 2$, then by part a the original density should be
normaliseable, so the resultant likelihood will also be normaliseable.

### Part d

```{r}
#| echo: false

log_posterior <- function(theta){
  delta <- theta[1]
  sigma <- theta[2]
  beta <- theta[3]
  
  #Heaviside step function in the prior
  if (delta <=0 || B<=2) return (-Inf)
  
  
  
}




log_posterior <- function(theta){
  p<-theta[1]
  q<-theta[2]
  
  #The Uniform Prior on  parameters p and q.
  if (p <= 0 || p >= 1 || q <= 0 || q >= 1) return(-Inf)
  
  #The log of the posterior distribution up to normalisation constant
  log_post <- Nsc*log(p) + (Ns-Nsc)*log(1-p) + Nnsc*log(q) + (Nns - Nnsc)*log(1-q)
  return (log_post)
}
```

# Question 4

### Part a

```{r}

#smoking_dat is our dataset

```

# Code Appendix {#sec-code-appendix}

```{r ref.label=knitr::all_labels()}
#| echo: true
#| eval: false
#| code-fold: true

```
